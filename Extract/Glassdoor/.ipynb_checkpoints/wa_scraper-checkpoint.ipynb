{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('windows/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.com/Job/washington-data-scientist-jobs-SRCH_IL.0,4_IS2235_KO5,19.htm?radius=25&fromAge=30'\n",
    "browser.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def geturls():\n",
    "#     url_set = set()\n",
    "#     base_url = 'https://www.glassdoor.com'\n",
    "#     pages = soup.find_all('li', class_='page')\n",
    "\n",
    "#     for page in pages:\n",
    "#         if page.a:\n",
    "#             url = base_url + page.a['href']\n",
    "#             if (url not in url_set) :\n",
    "#                  url_set.add(url)\n",
    "               \n",
    "#     return url_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_and_close_popup(browser):\n",
    "    \n",
    "    xpath= '//*[@id=\"MainCol\"]/div/ul/li[1]'\n",
    "    browser.find_element_by_xpath(xpath).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    xpath ='//*[@id=\"JAModal\"]/div/div/div[2]/div/div[1]/div/span'\n",
    "    browser.find_element_by_xpath(xpath).click()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_for_click(xpath):\n",
    "    browser.find_element_by_xpath(xpath).click()\n",
    "    time.sleep(1)\n",
    "    html = browser.page_source\n",
    "    return BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continue_bool = True\n",
    "jobs = []\n",
    "\n",
    "while True:\n",
    "\n",
    "    #trigger popup and close it\n",
    "    try:\n",
    "        trigger_and_close_popup(browser)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "#     close any possible popups with a Close button\n",
    "    try:\n",
    "        browser.find_element_by_partial_link_text('Close').click()\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "#     more popups\n",
    "    try:\n",
    "        browser.find_element_by_class_name('ModalStyle__xBtn___29PT9').click()\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    article = soup.find('article', id='MainCol').ul\n",
    "    job_list = article.find_all('li')\n",
    " \n",
    "    for job  in job_list:\n",
    "    #     title\n",
    "        title = job['data-normalize-job-title'].strip()\n",
    "    #     job location\n",
    "        location  = job['data-job-loc'].strip()\n",
    "    #     employer\n",
    "        employer = job.find('div', class_='jobEmpolyerName').text\n",
    "      \n",
    "    #     job details\n",
    "        dataId = job['data-id']\n",
    "        Id = 'JobDesc' + dataId\n",
    "        xpath = f\"//li[@data-id='{dataId}']\"\n",
    "        try:\n",
    "            soup = soup_for_click(xpath)\n",
    "            job_desc = soup.find('div', id = Id, class_=\"jobDesc\").text\n",
    "        except Exception as e:\n",
    "            job_desc = \"\" \n",
    "\n",
    "    #     review count\n",
    "        xpath= '//*[@id=\"Details\"]/div[1]/div[1]/div/div/div[5]'\n",
    "        try:\n",
    "            soup = soup_for_click(xpath)\n",
    "            reviews= soup.select(\"li.empReview\")\n",
    "            review_count = len(reviews)\n",
    "        except Exception as e:\n",
    "            review_count = 0\n",
    "\n",
    "    #   create the job dictoanry object and add it to the list\n",
    "        job_dict = {\"position\": title,\"company\": employer, \"description\": job_desc, \"reviews\": review_count, \\\n",
    "                    \"location\":location}\n",
    "\n",
    "        jobs.append(job_dict)\n",
    "        \n",
    "    try:\n",
    "        xpath = '//*[@id=\"FooterPageNav\"]/div/ul/li[7]/a'\n",
    "        browser.find_element_by_xpath(xpath).click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "\n",
    "print(\"Scraping Complete\")\n",
    "      \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to excel without the df index\n",
    "glassdoor_df = pd.DataFrame(jobs)\n",
    "glassdoor_df.to_excel(\"C:/Users/indup/ETL-Project/Transform/Resources/Glassdoor/wa_glassdoor.xlsx\", index=False)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
